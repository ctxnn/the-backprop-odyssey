{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0bd1c4",
   "metadata": {},
   "source": [
    "# a very basic neural network formulation and implementation\n",
    "\n",
    "note: the implementation will be using numpy only "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327a3e0",
   "metadata": {},
   "source": [
    "the forward process will go as follows: \n",
    "$$Z_1 = W_1 * x + B_1$$\n",
    "$$Z_1 = max(0, Z_1)$$\n",
    "$$Z_2 = W_2 * z_1 + B_2$$\n",
    "$$output = \\text{softmax}(Z_2)$$\n",
    "\n",
    "the backward process will go as follows: \n",
    "**Step 1: Compute loss gradient w.r.t. output**\n",
    "$$\\frac{\\partial L}{\\partial \\text{output}} = \\text{output} - \\text{one\\_hot}(y)$$\n",
    "\n",
    "**Step 2: Gradient w.r.t. Z₂ (softmax derivative)**\n",
    "$$\\frac{\\partial L}{\\partial Z_2} = \\frac{\\partial L}{\\partial \\text{output}} \\cdot \\frac{\\partial \\text{output}}{\\partial Z_2} = \\text{output} - \\text{one\\_hot}(y)$$\n",
    "\n",
    "**Step 3: Gradients w.r.t. W₂ and B₂**\n",
    "$$\\frac{\\partial L}{\\partial W_2} = Z_1^T \\cdot \\frac{\\partial L}{\\partial Z_2}$$\n",
    "$$\\frac{\\partial L}{\\partial B_2} = \\sum \\frac{\\partial L}{\\partial Z_2}$$\n",
    "\n",
    "**Step 4: Gradient w.r.t. Z₁ (chain rule)**\n",
    "$$\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial Z_2} \\cdot W_2^T \\cdot \\frac{\\partial \\text{ReLU}}{\\partial Z_1}$$\n",
    "$$\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial Z_2} \\cdot W_2^T \\cdot (Z_1 > 0)$$\n",
    "\n",
    "**Step 5: Gradients w.r.t. W₁ and B₁**\n",
    "$$\\frac{\\partial L}{\\partial W_1} = x^T \\cdot \\frac{\\partial L}{\\partial Z_1}$$\n",
    "$$\\frac{\\partial L}{\\partial B_1} = \\sum \\frac{\\partial L}{\\partial Z_1}$$\n",
    "\n",
    "**Step 6: Parameter updates**\n",
    "$$W_2 := W_2 - \\alpha \\frac{\\partial L}{\\partial W_2}$$\n",
    "$$B_2 := B_2 - \\alpha \\frac{\\partial L}{\\partial B_2}$$\n",
    "$$W_1 := W_1 - \\alpha \\frac{\\partial L}{\\partial W_1}$$\n",
    "$$B_1 := B_1 - \\alpha \\frac{\\partial L}{\\partial B_1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730cdf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP: \n",
    "    \n",
    "    def __init__(self, dims, lr):\n",
    "        self.dims = dims \n",
    "        self.lr = lr \n",
    "        \n",
    "        #dims = [input dim, hidden dim, output dim]\n",
    "        self.W1 = np.random.randn(dims[0], dims[1]) \n",
    "        self.b1 = np.zeros((1, dims[1]))\n",
    "        self.W2 = np.random.randn(dims[1], dims[2])\n",
    "        self.b2 = np.zeros((1, dims[2]))\n",
    "        \n",
    "    def activation(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.activation(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        log_likelihood = -np.log(y_pred[range(m), y_true])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss \n",
    "    \n",
    "    def backward(self, x):\n",
    "        m = x.shape[0]\n",
    "        delta2 = self.a2.copy()\n",
    "        delta2[range(m), x] -= 1\n",
    "        delta2 /= m\n",
    "\n",
    "        dW2 = np.dot(self.a1.T, delta2)\n",
    "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
    "\n",
    "        delta1 = np.dot(delta2, self.W2.T) * (self.z1 > 0)\n",
    "        dW1 = np.dot(x.T, delta1)\n",
    "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
    "\n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def update(self, dW1, db1, dW2, db2):\n",
    "        self.W1 -= self.lr * dW1\n",
    "        self.b1 -= self.lr * db1\n",
    "        self.W2 -= self.lr * dW2\n",
    "        self.b2 -= self.lr * db2\n",
    "        \n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X\n",
    "            loss = self.loss(y, y_pred)\n",
    "            dW1, db1, dW2, db2 = self.backward(y)\n",
    "            self.update(dW1, db1, dW2, db2)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "                \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0941e317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.5474\n",
      "Epoch 100, Loss: 0.9780\n",
      "Epoch 200, Loss: 0.9054\n",
      "Epoch 300, Loss: 0.8757\n",
      "Epoch 400, Loss: 0.8528\n",
      "Epoch 500, Loss: 0.8312\n",
      "Epoch 600, Loss: 0.8117\n",
      "Epoch 700, Loss: 0.7945\n",
      "Epoch 800, Loss: 0.7797\n",
      "Epoch 900, Loss: 0.7646\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "X = np.random.rand(100, 20)\n",
    "y = np.random.randint(0, 2, size=100)\n",
    "\n",
    "# Initialize model\n",
    "model = MLP(dims=[20, 10, 2], lr=0.001)\n",
    "\n",
    "# Train model\n",
    "model.train(X, y, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fb2d08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: node 'softmax', graph '%1' size too small for label\n",
      "Warning: node 'softmax', graph '%1' size too small for label\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'computational_graph_full.png'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def draw_full_computational_graph():\n",
    "    dot = Digraph(format='png', graph_attr={\n",
    "        'rankdir': 'TB',\n",
    "        'splines': 'spline',\n",
    "        'nodesep': '0.6',\n",
    "        'ranksep': '0.8'\n",
    "    })\n",
    "\n",
    "    # ----- Styles -----\n",
    "    param_style = {'shape': 'record', 'style': 'rounded,filled', 'fillcolor': 'lightyellow'}\n",
    "    op_style = {'shape': 'circle', 'style': 'filled', 'fillcolor': 'lightblue', 'width': '0.5', 'fixedsize': 'true'}\n",
    "    value_style = {'shape': 'record', 'style': 'rounded,filled', 'fillcolor': 'lightgrey'}\n",
    "\n",
    "    # ----- Input Layer -----\n",
    "    dot.attr('node', **param_style)\n",
    "    dot.node('X', '{X | data | grad: ∂L/∂X}')\n",
    "    dot.node('y', '{y | labels}')\n",
    "    dot.node('W1', '{W1 | params | grad: ∂L/∂W1 = Xᵀ·∂L/∂Z1}')\n",
    "    dot.node('b1', '{b1 | params | grad: ∂L/∂b1 = Σ∂L/∂Z1}')\n",
    "    dot.node('W2', '{W2 | params | grad: ∂L/∂W2 = A1ᵀ·∂L/∂Z2}')\n",
    "    dot.node('b2', '{b2 | params | grad: ∂L/∂b2 = Σ∂L/∂Z2}')\n",
    "\n",
    "    # ----- Operation Nodes -----\n",
    "    dot.attr('node', **op_style)\n",
    "    dot.node('dot1', '*')\n",
    "    dot.node('add1', '+')\n",
    "    dot.node('relu', 'ReLU')\n",
    "    dot.node('dot2', '*')\n",
    "    dot.node('add2', '+')\n",
    "    dot.node('softmax', 'Softmax')\n",
    "    dot.node('loss', 'Loss')\n",
    "\n",
    "    # ----- Intermediate Values with Forward + Backward -----\n",
    "    dot.attr('node', **value_style)\n",
    "    dot.node('Z1', '''{Z1 | forward: Z1 = X·W1 + b1 | backward: ∂L/∂Z1 = (∂L/∂Z2·W2ᵀ) ⊙ ReLU'(Z1)}''')\n",
    "    dot.node('A1', '''{A1 | forward: A1 = ReLU(Z1) | backward: ∂L/∂A1 = ∂L/∂Z2·W2ᵀ}''')\n",
    "    dot.node('Z2', '''{Z2 | forward: Z2 = A1·W2 + b2 | backward: ∂L/∂Z2 = A2 - y}''')\n",
    "    dot.node('A2', '''{A2 | forward: A2 = softmax(Z2) | backward: ∂L/∂A2 = A2 - y}''')\n",
    "    dot.node('L', '''{L | forward: CrossEntropy | backward: ∂L/∂L = 1}''')\n",
    "\n",
    "    # ----- Forward Pass Edges -----\n",
    "    dot.edge('X', 'dot1')\n",
    "    dot.edge('W1', 'dot1')\n",
    "    dot.edge('dot1', 'add1')\n",
    "    dot.edge('b1', 'add1')\n",
    "    dot.edge('add1', 'Z1')\n",
    "    dot.edge('Z1', 'relu')\n",
    "    dot.edge('relu', 'A1')\n",
    "    dot.edge('A1', 'dot2')\n",
    "    dot.edge('W2', 'dot2')\n",
    "    dot.edge('dot2', 'add2')\n",
    "    dot.edge('b2', 'add2')\n",
    "    dot.edge('add2', 'Z2')\n",
    "    dot.edge('Z2', 'softmax')\n",
    "    dot.edge('softmax', 'A2')\n",
    "    dot.edge('A2', 'loss')\n",
    "    dot.edge('y', 'loss')\n",
    "    dot.edge('loss', 'L')\n",
    "\n",
    "    return dot\n",
    "\n",
    "# Render\n",
    "graph = draw_full_computational_graph()\n",
    "graph.render('computational_graph_full', format='png', cleanup=True)\n",
    "graph.view()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
